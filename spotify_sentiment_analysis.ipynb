{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Dataset Sentiment Analysis using NaÃ¯ve Bayes\n",
    "\n",
    "This notebook implements sentiment classification using MultinomialNB with comprehensive text preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kagglehub for dataset loading\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download the Spotify dataset from Kaggle\n",
    "print(\"Downloading Spotify dataset from Kaggle...\")\n",
    "try:\n",
    "    # Download the dataset files to local directory\n",
    "    path = kagglehub.dataset_download(\"alexandrakim2201/spotify-dataset\")\n",
    "    print(f\"Dataset downloaded to: {path}\")\n",
    "    \n",
    "    # List files in the downloaded directory\n",
    "    files = os.listdir(path)\n",
    "    print(f\"Available files: {files}\")\n",
    "    \n",
    "    # Find the CSV file (should be the main dataset)\n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        csv_file = csv_files[0]  # Take the first CSV file\n",
    "        file_path = os.path.join(path, csv_file)\n",
    "        print(f\"Loading CSV file: {csv_file}\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "    else:\n",
    "        print(\"No CSV files found in the dataset\")\n",
    "        # Fallback: try to load any file as CSV\n",
    "        if files:\n",
    "            file_path = os.path.join(path, files[0])\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Loaded {files[0]} as CSV\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No files found in dataset\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading from Kaggle: {e}\")\n",
    "    print(\"Please ensure you have Kaggle API credentials set up.\")\n",
    "    print(\"Alternatively, download the dataset manually and update the path below.\")\n",
    "    \n",
    "    # Fallback: try to load from local file if it exists\n",
    "    local_file = \"spotify_dataset.csv\"\n",
    "    if os.path.exists(local_file):\n",
    "        print(f\"Loading from local file: {local_file}\")\n",
    "        df = pd.read_csv(local_file)\n",
    "    else:\n",
    "        print(\"Please download the dataset manually and place it as 'spotify_dataset.csv'\")\n",
    "        raise\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataset description:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Data Preprocessing and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Explore the target variable (sentiment)\n",
    "# Dataset has 51,473 rows with columns: 'Review' (text) and 'label' (POSITIVE/NEGATIVE)\n",
    "sentiment_column = 'label'  # Sentiment labels: POSITIVE/NEGATIVE\n",
    "text_column = 'Review'  # User review text\n",
    "\n",
    "if sentiment_column in df.columns:\n",
    "    print(\"Sentiment distribution:\")\n",
    "    print(df[sentiment_column].value_counts())\n",
    "    \n",
    "    # Visualize sentiment distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df[sentiment_column].value_counts().plot(kind='bar')\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Please update the sentiment_column variable with the correct column name\")\n",
    "    print(\"Available columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Text Preprocessing - Initialize Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text preprocessing libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Preprocessing tools initialized!\")"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase (case folding)\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenization\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stop words\"\"\"\n",
    "    return [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatization\"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def preprocess_text(text, use_stemming=True, use_lemmatization=False):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    if use_stemming:\n",
    "        tokens = stem_tokens(tokens)\n",
    "    elif use_lemmatization:\n",
    "        tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Text preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Apply Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text data\n",
    "# Update 'text_column' with the actual column name containing text data\n",
    "if text_column in df.columns:\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    \n",
    "    # Create processed text column\n",
    "    df['processed_text'] = df[text_column].apply(lambda x: preprocess_text(x, use_stemming=True))\n",
    "    \n",
    "    # Show examples of original vs processed text\n",
    "    print(\"\\nExample of text preprocessing:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\nOriginal: {df[text_column].iloc[i][:100]}...\")\n",
    "        print(f\"Processed: {df['processed_text'].iloc[i][:100]}...\")\n",
    "    \n",
    "    # Remove empty processed texts\n",
    "    df = df[df['processed_text'].str.len() > 0]\n",
    "    print(f\"\\nDataset shape after preprocessing: {df.shape}\")\n",
    "else:\n",
    "    print(\"Please update the text_column variable with the correct column name\")\n",
    "    print(\"Available columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF-IDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit to top 5000 features\n",
    "    min_df=2,          # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95,       # Ignore terms that appear in more than 95% of documents\n",
    "    ngram_range=(1, 2) # Use unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the processed text\n",
    "if 'processed_text' in df.columns:\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "    \n",
    "    print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "    print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    # Show top features\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "else:\n",
    "    print(\"Processed text not available. Please run the preprocessing step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and target variables\n",
    "if sentiment_column in df.columns and 'processed_text' in df.columns:\n",
    "    X = X_tfidf\n",
    "    y = df[sentiment_column]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    print(f\"\\nTraining set sentiment distribution:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"\\nTesting set sentiment distribution:\")\n",
    "    print(y_test.value_counts())\n",
    "else:\n",
    "    print(\"Please ensure both sentiment and processed text columns are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Train MultinomialNB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MultinomialNB classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize and train MultinomialNB classifier\n",
    "nb_classifier = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "\n",
    "# Train the model\n",
    "print(\"Training MultinomialNB classifier...\")\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Number of classes: {len(nb_classifier.classes_)}\")\n",
    "print(f\"Classes: {nb_classifier.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "y_pred_proba = nb_classifier.predict_proba(X_test)\n",
    "\n",
    "print(\"Predictions completed!\")\n",
    "print(f\"Predicted sentiment distribution:\")\n",
    "print(pd.Series(y_pred).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Model Evaluation - Accuracy and Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate basic evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"=== Model Performance Metrics ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "\n",
    "# Create a summary dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate detailed classification report\n",
    "print(\"=== Detailed Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Convert to dataframe for better visualization\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "print(\"\\nClassification Report as DataFrame:\")\n",
    "print(report_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion matrix and seaborn for visualization\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate and visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = nb_classifier.classes_\n",
    "\n",
    "# Create confusion matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix as dataframe\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14: Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most important features for each class\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "n_features = 10  # Top 10 features per class\n",
    "\n",
    "print(\"=== Most Important Features by Class ===\")\n",
    "for i, class_label in enumerate(nb_classifier.classes_):\n",
    "    # Get feature log probabilities for this class\n",
    "    feature_log_prob = nb_classifier.feature_log_prob_[i]\n",
    "    \n",
    "    # Get top features\n",
    "    top_features_idx = np.argsort(feature_log_prob)[-n_features:]\n",
    "    top_features = [(feature_names[idx], feature_log_prob[idx]) for idx in top_features_idx]\n",
    "    \n",
    "    print(f\"\\nClass: {class_label}\")\n",
    "    for feature, prob in reversed(top_features):\n",
    "        print(f\"  {feature}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15: Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Metrics bar plot\n",
    "metrics_df.plot(x='Metric', y='Score', kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Model Performance Metrics')\n",
    "axes[0,0].set_ylabel('Score')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Class-wise performance\n",
    "class_metrics = report_df.iloc[:-3, :3]  # Exclude avg rows and support column\n",
    "class_metrics.plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Class-wise Performance')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].legend(['Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "# 3. Prediction distribution\n",
    "pred_dist = pd.Series(y_pred).value_counts()\n",
    "pred_dist.plot(kind='pie', ax=axes[1,0], autopct='%1.1f%%')\n",
    "axes[1,0].set_title('Predicted Sentiment Distribution')\n",
    "axes[1,0].set_ylabel('')\n",
    "\n",
    "# 4. True vs Predicted comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True': y_test.value_counts(),\n",
    "    'Predicted': pd.Series(y_pred).value_counts()\n",
    "})\n",
    "comparison_df.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('True vs Predicted Distribution')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 16: Sample Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze some sample predictions\n",
    "sample_indices = np.random.choice(len(X_test), size=5, replace=False)\n",
    "\n",
    "print(\"=== Sample Predictions Analysis ===\")\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    original_idx = X_test.indices[idx] if hasattr(X_test, 'indices') else idx\n",
    "    \n",
    "    true_label = y_test.iloc[idx]\n",
    "    pred_label = y_pred[idx]\n",
    "    pred_proba = y_pred_proba[idx]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {pred_label}\")\n",
    "    print(f\"Prediction Confidence: {max(pred_proba):.4f}\")\n",
    "    \n",
    "    # Show probabilities for all classes\n",
    "    for j, class_label in enumerate(nb_classifier.classes_):\n",
    "        print(f\"  P({class_label}): {pred_proba[j]:.4f}\")\n",
    "    \n",
    "    print(f\"Correct: {'â' if true_label == pred_label else 'â'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 17: Model Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print(\"=== MODEL SUMMARY ===\")\n",
    "print(f\"Dataset: Spotify Sentiment Analysis\")\n",
    "print(f\"Algorithm: Multinomial NaÃ¯ve Bayes\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Number of features: {X_tfidf.shape[1]}\")\n",
    "print(f\"Number of classes: {len(nb_classifier.classes_)}\")\n",
    "\n",
    "print(\"\\n=== PREPROCESSING TECHNIQUES APPLIED ===\")\n",
    "print(\"â Tokenization\")\n",
    "print(\"â Case folding (lowercase conversion)\")\n",
    "print(\"â Punctuation removal\")\n",
    "print(\"â Stop words removal\")\n",
    "print(\"â Stemming\")\n",
    "print(\"â TF-IDF Vectorization\")\n",
    "\n",
    "print(\"\\n=== FINAL PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Recall: {recall:.4f}\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "if accuracy > 0.8:\n",
    "    print(\"â Model shows good performance\")\n",
    "elif accuracy > 0.7:\n",
    "    print(\"â  Model shows moderate performance - consider feature engineering\")\n",
    "else:\n",
    "    print(\"â  Model needs improvement - try different preprocessing or algorithms\")\n",
    "\n",
    "print(\"\\nAnalysis completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}